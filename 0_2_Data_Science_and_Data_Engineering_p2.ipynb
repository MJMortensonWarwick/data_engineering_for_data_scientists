{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd8raq+t5nERUvBcUQHVJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/data_engineering_for_data_scientists/blob/main/0_2_Data_Science_and_Data_Engineering_p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.2 Data Science and Data Engineering (part two)\n",
        "The previous notebook ([0.1](https://github.com/MJMortensonWarwick/data_engineering_for_data_scientists/blob/main/0_1_Data_Science_and_Data_Engineering_p1.ipynb)) shows some of the issues you might find in a Data Scientist's Notebook (compared to how a Data Engineer might approach this). This Notebook will shows you an alternative implementation!"
      ],
      "metadata": {
        "id": "Pgkz60hfWP7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8hHTigol9Ro"
      },
      "outputs": [],
      "source": [
        "# Import libraries - would be handled in a requirements.txt file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than a bunch of `print()` statements, our data engineering approach will write out to `logging`. This means we can track any issues in the standalone log rather than relying on catching any mistake while the code runs.\n",
        "\n",
        "We also will create a `Config` that will store the paths we need for data and the constants such as exchange rate and so on. By making these independent, they are easier to control and change. In practice we would not include this in the code but in the overall platform (as stored 'secrets'), but that is hard to illustrate here."
      ],
      "metadata": {
        "id": "7Dvlyet0WyFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logging (instead of print statements)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Use a config class (no more magic numbers)\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Paths (Ideally these come from env vars, but relative paths are better than absolute)\n",
        "    INPUT_PATH: str = \"data/raw/sales_data.csv\"\n",
        "    OUTPUT_PATH: str = \"data/processed/scored_leads_{date}.csv\"\n",
        "\n",
        "    # Business Logic Constants\n",
        "    FX_RATE_GBP_USD: float = 0.82\n",
        "    EXCLUDED_REGIONS: list = (4,)\n",
        "\n",
        "    # Campaign Multipliers\n",
        "    CLICK_MULTIPLIER_A: float = 0.05\n",
        "    CLICK_MULTIPLIER_B: float = 0.12\n",
        "\n",
        "    # Outlier Capping\n",
        "    MAX_SCORE_CAP: int = 1000\n",
        "    CAP_VALUE: int = 999\n",
        "\n",
        "    # Model Params\n",
        "    TEST_SPLIT_SIZE: float = 0.2\n",
        "    RANDOM_SEED: int = 42\n",
        "\n",
        "config = Config()\n",
        "logger.info(\"Configuration loaded.\")"
      ],
      "metadata": {
        "id": "O7ViTPIhmEES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than a large block of code which is hard to understand and not reusable, instead we will favour a series of functions. Each of these will have clearly defined scope (ideally performing a single task), set of inputs and produce a clear output. In each function we will state these at the very start. Each process will report its output directly to the logger."
      ],
      "metadata": {
        "id": "8iccawKsXTL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "~def load_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads data and handles missing file errors gracefully.\n",
        "       INPUTS: a dataframe path\n",
        "       OUTPUTS: a dataframe\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        logger.info(f\"Data loaded successfully: {df.shape[0]} rows\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"File not found at {path}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "Ee9UaMh2mIcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also favour speed. Our previous code included a for loop - `.iterrows()`. This is fine in experimentation and with a limited data size, but is a relatively slow approach as it needs to process each row one by one. Vectorising the operation (as below) means the whole dataset is transformed in a single operation. As a data engineer, even if the dataset we first work with is relatively small, we should be designing code that can easily scale up to much larger datasets."
      ],
      "metadata": {
        "id": "O0b_Y6yoX9t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
        "    \"\"\"Applies standard cleaning and filtering rules.\n",
        "       INPUTS: a dataframe\n",
        "       OUTPUTS: a new dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorised operation (fast) instead of .iterrows (slow)\n",
        "    df['adjusted_price'] = df['raw_price'] * cfg.FX_RATE_GBP_USD\n",
        "\n",
        "    # Filter using readable syntax\n",
        "    initial_count = len(df)\n",
        "    df = df[~df['region_id'].isin(cfg.EXCLUDED_REGIONS)]\n",
        "    removed = initial_count - len(df)\n",
        "\n",
        "    logger.info(f\"Cleaned data. Removed {removed} rows from excluded regions.\")\n",
        "    return df.copy() # Return a copy to avoid SettingWithCopy warnings"
      ],
      "metadata": {
        "id": "IL3l4b1Rmb6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_score(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
        "    \"\"\"Engineers the 'score' feature based on campaign type.\n",
        "       INPUTS: a dataframe\n",
        "       OUTPUTS: a new dataframe\n",
        "    \"\"\"\n",
        "    # Vectorised logic using numpy select (much faster than loops)\n",
        "    conditions = [\n",
        "        df['type'] == 'A',\n",
        "        df['type'] == 'B'\n",
        "    ]\n",
        "    choices = [\n",
        "        df['clicks'] * cfg.CLICK_MULTIPLIER_A,\n",
        "        df['clicks'] * cfg.CLICK_MULTIPLIER_B\n",
        "    ]\n",
        "\n",
        "    df['score'] = np.select(conditions, choices, default=0)\n",
        "\n",
        "    # Handle outliers\n",
        "    outlier_count = df[df['score'] > cfg.MAX_SCORE_CAP].shape[0]\n",
        "    df['score'] = np.where(df['score'] > cfg.MAX_SCORE_CAP, cfg.CAP_VALUE, df['score'])\n",
        "\n",
        "    if outlier_count > 0:\n",
        "        logger.warning(f\"Capped {outlier_count} scores > {cfg.MAX_SCORE_CAP} to {cfg.CAP_VALUE}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "6adXk_LemoCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(df: pd.DataFrame, cfg: Config):\n",
        "    \"\"\"Splits data and trains the model reproducibly.\n",
        "       INPUTS: a dataframe\n",
        "       OUTPUTS: a trained model\n",
        "    \"\"\"\n",
        "\n",
        "    X = df[['score']]\n",
        "    y = df['conversions']\n",
        "\n",
        "    # Reproducible split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=cfg.TEST_SPLIT_SIZE, random_state=cfg.RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    score = model.score(X_test, y_test)\n",
        "    logger.info(f\"Model Training Complete. R^2 Score: {score:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZcuZMLBRmw5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With all our operations now written out as independent functions, our final job is to orchestrate this as a flow. The `if __name__ == __main__` instruction means this part of the code will operate if the script is called remotely (as opposed to being manually run).  "
      ],
      "metadata": {
        "id": "CnJrK7ugYm6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This orchestrates the flow. In a real system, this might be an Airflow DAG.\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Extract\n",
        "    raw_df = load_data(config.INPUT_PATH)\n",
        "\n",
        "    # 2. Transform\n",
        "    clean_df = clean_data(raw_df, config)\n",
        "    final_df = calculate_score(clean_df, config)\n",
        "\n",
        "    # 3. Model\n",
        "    model = train_model(final_df, config)\n",
        "\n",
        "    # 4. Load / Save\n",
        "    from datetime import datetime\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "    output_filename = config.OUTPUT_PATH.format(date=timestamp)\n",
        "\n",
        "    final_df.to_csv(output_filename, index=False)\n",
        "    logger.info(f\"Results saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "aN1XVDVIm8lX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}