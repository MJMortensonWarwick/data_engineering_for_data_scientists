{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN57W3aL8J2rvuBgQQ9D1rN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/data_engineering_for_data_scientists/blob/main/1_2_Medallion_Architecture_Data_Lakehouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Medallion Architecture / Data Lakehouse\n",
        "This Notebook works similarly to our ETL Notebook, but explicity creates three layers to our data lake (bronze, silver, gold).\n",
        "\n",
        "We will relax some of the previous rigour to make this a more natural Notebook progression (e.g. no `utilities.py` file and so on).\n",
        "\n",
        "We'll start by creating some raw data and saving it to the bronze layer (which we will also make).\n"
      ],
      "metadata": {
        "id": "Gn2ZrFzfuE58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IcVvcBlt-jK",
        "outputId": "0cec3456-d17b-4025-e6da-dbdee0a80383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRONZE LAYER: Generated 1020 raw records at datalake/bronze/raw_transactions.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import duckdb\n",
        "\n",
        "# Create our \"Data Lake\" directories\n",
        "os.makedirs(\"datalake/bronze\", exist_ok=True)\n",
        "os.makedirs(\"datalake/silver\", exist_ok=True)\n",
        "os.makedirs(\"datalake/gold\", exist_ok=True)\n",
        "\n",
        "# 1. Generate \"Messy\" Raw Data\n",
        "def generate_bronze_data():\n",
        "    np.random.seed(42)\n",
        "    data = []\n",
        "\n",
        "    # We simulate 1000 raw events\n",
        "    for i in range(1000):\n",
        "        event = {\n",
        "            \"transaction_id\": str(i) if np.random.rand() > 0.05 else None, # 5% missing IDs\n",
        "            \"user_id\": np.random.randint(100, 110), # Only 10 users\n",
        "            \"amount\": np.random.choice([10.5, 20.0, \"50.0\", -5.0, None]), # Mixed types & bad data\n",
        "            \"timestamp\": \"2024-02-10T10:00:00\" if i % 2 == 0 else \"2024-02-10 10:01:00\", # Inconsistent dates\n",
        "            \"raw_metadata\": \"{\\\"source\\\": \\\"mobile\\\"}\" # Nested JSON string\n",
        "        }\n",
        "        # Duplicate some rows to simulate ingestion errors\n",
        "        if i % 50 == 0:\n",
        "            data.append(event)\n",
        "        data.append(event)\n",
        "\n",
        "    # Save as Raw JSON (Line Delimited) - The \"Bronze\" format\n",
        "    # Bronze is often just a \"dump\" of the source system.\n",
        "    file_path = \"datalake/bronze/raw_transactions.json\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        for entry in data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "    print(f\"BRONZE LAYER: Generated {len(data)} raw records at {file_path}\")\n",
        "\n",
        "generate_bronze_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bronze to Silver (Cleaning & Validation)\n",
        "Goal: Clean the data, enforce types, and deduplicate. Save as Parquet.\n",
        "\n",
        "Why Parquet? It compresses data and stores schema information, making it much faster to read than JSON."
      ],
      "metadata": {
        "id": "RxfWxVTHut8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_silver():\n",
        "    print(\"\\nPROCESSING: Bronze -> Silver\")\n",
        "\n",
        "    # 1. Read Raw Data\n",
        "    df = pd.read_json(\"datalake/bronze/raw_transactions.json\", lines=True)\n",
        "\n",
        "    # 2. Data Cleaning Rules\n",
        "    # Drop duplicates\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    # Fix 'amount': Convert to numeric, coerce errors (strings) to NaN, fill NaNs with 0\n",
        "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors='coerce').fillna(0)\n",
        "\n",
        "    # Filter out negative amounts (Business Rule)\n",
        "    df = df[df[\"amount\"] >= 0]\n",
        "\n",
        "    # Fix Timestamp format\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "\n",
        "    # 3. Enforce Schema & Save as Parquet\n",
        "    # This ensures that downstream users don't have to guess data types.\n",
        "    output_path = \"datalake/silver/clean_transactions.parquet\"\n",
        "    df.to_parquet(output_path, index=False)\n",
        "\n",
        "    print(f\"SILVER LAYER: Saved {len(df)} clean records to {output_path}\")\n",
        "    print(df.head(3))\n",
        "\n",
        "process_silver()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3swoEg0u0gk",
        "outputId": "bc503737-2b17-4383-8812-b41cda402e32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROCESSING: Bronze -> Silver\n",
            "SILVER LAYER: Saved 783 clean records to datalake/silver/clean_transactions.parquet\n",
            "   transaction_id  user_id  amount           timestamp          raw_metadata\n",
            "0             0.0      107     0.0 2024-02-10 10:00:00  {\"source\": \"mobile\"}\n",
            "2             1.0      109    50.0 2024-02-10 10:01:00  {\"source\": \"mobile\"}\n",
            "3             2.0      107     0.0 2024-02-10 10:00:00  {\"source\": \"mobile\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Silver to Gold (Aggregation & Business Logic)\n",
        "Goal: Create a business-level aggregate table (e.g., \"Daily Sales per User\").\n",
        "\n",
        "This layer is highly curated. It is usually \"read-optimised\" for dashboards or engineered features for data science models."
      ],
      "metadata": {
        "id": "qDhAxSt1u8G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_gold():\n",
        "    print(\"\\nPROCESSING: Silver -> Gold\")\n",
        "\n",
        "    # 1. Read Silver Data (Fast Parquet Read)\n",
        "    df = pd.read_parquet(\"datalake/silver/clean_transactions.parquet\")\n",
        "\n",
        "    # 2. Aggregation (The \"Business Logic\")\n",
        "    gold_df = df.groupby(\"user_id\").agg(\n",
        "        total_spent=(\"amount\", \"sum\"),\n",
        "        transaction_count=(\"transaction_id\", \"count\"),\n",
        "        last_seen=(\"timestamp\", \"max\")\n",
        "    ).reset_index()\n",
        "\n",
        "    # 3. Save to Gold\n",
        "    output_path = \"datalake/gold/user_daily_summary.parquet\"\n",
        "    gold_df.to_parquet(output_path, index=False)\n",
        "\n",
        "    print(f\"GOLD LAYER: Saved {len(gold_df)} summary records to {output_path}\")\n",
        "    print(gold_df.head(3))\n",
        "\n",
        "process_gold()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7dVLujTvMZw",
        "outputId": "53e82864-b6d3-4bd8-e9ff-5d10561015b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PROCESSING: Silver -> Gold\n",
            "GOLD LAYER: Saved 10 summary records to datalake/gold/user_daily_summary.parquet\n",
            "   user_id  total_spent  transaction_count           last_seen\n",
            "0      100       1609.0                 71 2024-02-10 10:01:00\n",
            "1      101       1405.5                 61 2024-02-10 10:01:00\n",
            "2      102       1688.0                 78 2024-02-10 10:01:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Lakehouse\n",
        "Lastly, we will build a data lakehouse using DuckDB. Note, this is a virtual database that does not actually store the data. Instead we infer the schema, and return data, from the parquet files we saved in the Gold layer."
      ],
      "metadata": {
        "id": "tWAxcEHevV0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nANALYST QUERY (DuckDB Lakehouse)\")\n",
        "\n",
        "# We use DuckDB to query the file directly.\n",
        "# This mimics a \"Serverless SQL\" experience.\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT\n",
        "        user_id,\n",
        "        total_spent,\n",
        "        CASE\n",
        "            WHEN total_spent > 1000 THEN 'VIP'\n",
        "            ELSE 'Regular'\n",
        "        END as user_segment\n",
        "    FROM 'datalake/gold/user_daily_summary.parquet'\n",
        "    ORDER BY total_spent DESC\n",
        "    LIMIT 5;\n",
        "\"\"\"\n",
        "\n",
        "con = duckdb.connect(database=':memory:')\n",
        "con.sql(query).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZhPLNugvomm",
        "outputId": "1f861312-9417-43b8-c51e-bac271480dc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANALYST QUERY (DuckDB Lakehouse)\n",
            "┌─────────┬─────────────┬──────────────┐\n",
            "│ user_id │ total_spent │ user_segment │\n",
            "│  int64  │   double    │   varchar    │\n",
            "├─────────┼─────────────┼──────────────┤\n",
            "│     107 │      2082.0 │ VIP          │\n",
            "│     103 │      1701.5 │ VIP          │\n",
            "│     102 │      1688.0 │ VIP          │\n",
            "│     109 │      1661.5 │ VIP          │\n",
            "│     100 │      1609.0 │ VIP          │\n",
            "└─────────┴─────────────┴──────────────┘\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, this looks, feels and works like a normal datawarehouse or RDBMs just its sitting directly on top of our data files.\n",
        "\n",
        "## QUESTIONS\n",
        "\n",
        "\n",
        "1.   Why not just query the Bronze file directly?\n",
        "2.   Why do we need a Gold layer? Why not just aggregate Silver every time?\n",
        "3. If I find a bug in the Silver cleaning logic, what do I do?\n",
        "\n"
      ],
      "metadata": {
        "id": "rcMr2KXxvyHg"
      }
    }
  ]
}