{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsuFvlVgTn1CWdeBzwiXB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/data_engineering_for_data_scientists/blob/main/1_1_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Extract, Transform, Load (ETL)\n",
        "This Notebook will walk us through a full ETL process, following the best practices we have identified.\n",
        "\n",
        "Our first step will be to create a `utilities.py` that will hold all of our key functions. As before, we will follow some familiar principles:\n",
        "* logging to record all our outcomes;\n",
        "* a 'purpose', 'INPUT', 'OUTPUT' approach to describing our functions;\n",
        "* functions serving a single purpose with 'clean' names that are easy to recall;\n",
        "* general FP and cloud native principles."
      ],
      "metadata": {
        "id": "OKzO1up8pbPe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofY4XHTpTWI",
        "outputId": "61839042-1519-4bf2-e065-fb65da690922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utilities.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utilities.py\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ==========================================\n",
        "# 0. LOGGING SETUP\n",
        "# ==========================================\n",
        "# We configure a logger that looks like a production service\n",
        "logger = logging.getLogger(\"ETL_Service\")\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create a console handler if one doesn't exist\n",
        "if not logger.handlers:\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "# ==========================================\n",
        "# 1. EXTRACT (The Source)\n",
        "# ==========================================\n",
        "def extract_data_from_source() -> List[Dict]:\n",
        "    \"\"\"Simulates fetching data from an external API.\n",
        "       INPUT: None (as this is a simulation)\n",
        "       OUTPUT: A dataset\n",
        "    \"\"\"\n",
        "    logger.info(\"Connecting to external source...\")\n",
        "    data = [\n",
        "        {\"id\": 101, \"Product\": \"Laptop\", \"price\": \"$1200.00\", \"status\": \"completed\", \"date\": \"2023-10-01\"},\n",
        "        {\"id\": 102, \"Product\": \"Mouse\", \"price\": \"$25.50\", \"status\": \"refunded\", \"date\": \"2023-10-02\"},\n",
        "        {\"id\": 103, \"Product\": \"Monitor\", \"price\": \"300\", \"status\": \"completed\", \"date\": \"2023-10-03\"},\n",
        "        {\"id\": 104, \"Product\": \"HDMI Cable\", \"price\": \"$15.00\", \"status\": \"completed\", \"date\": \"2023-10-03\"},\n",
        "        {\"id\": 105, \"Product\": \"Keyboard\", \"price\": None, \"status\": \"error\", \"date\": \"2023-10-04\"},\n",
        "    ]\n",
        "    logger.info(f\"Extraction successful. Received {len(data)} records.\")\n",
        "    return data\n",
        "\n",
        "# ==========================================\n",
        "# 2. TRANSFORMATIONS (Pure Functions)\n",
        "# ==========================================\n",
        "def normalize_keys(row: Dict) -> Dict:\n",
        "    ''' transforms dictionary keys to lower case\n",
        "        INPUT: dictionary\n",
        "        OUTPUT: dictionary (lower case)\n",
        "    '''\n",
        "    return {k.lower(): v for k, v in row.items()}\n",
        "\n",
        "def clean_price(row: Dict) -> Dict:\n",
        "    ''' cleans price data for errors\n",
        "        INPUT: dictionary\n",
        "        OUTPUT: dictionary (cleaned)\n",
        "    '''\n",
        "    new_row = row.copy()\n",
        "    price_raw = row.get(\"price\")\n",
        "\n",
        "    try:\n",
        "        if isinstance(price_raw, str):\n",
        "            clean_val = float(price_raw.replace(\"$\", \"\"))\n",
        "            new_row[\"price\"] = clean_val\n",
        "            # Log specific data quality fixes (Verbose)\n",
        "            if \"$\" in price_raw:\n",
        "                logger.debug(f\"Fixed currency format for ID {row.get('id')}\")\n",
        "        elif price_raw is None:\n",
        "            new_row[\"price\"] = 0.0\n",
        "            logger.warning(f\"Found NULL price for ID {row.get('id')}. Defaulting to 0.0\")\n",
        "    except ValueError:\n",
        "        logger.error(f\"Failed to parse price for ID {row.get('id')}: {price_raw}\")\n",
        "        new_row[\"price\"] = 0.0\n",
        "\n",
        "    return new_row\n",
        "\n",
        "def is_valid_transaction(row: Dict) -> bool:\n",
        "    ''' validates transaction data\n",
        "        INPUT: dictionary\n",
        "        OUTPUT: boolean (valid or not)\n",
        "    '''\n",
        "    is_valid = row.get(\"status\") == \"completed\" and row.get(\"price\", 0) > 0\n",
        "    if not is_valid:\n",
        "        logger.info(f\"Dropping invalid row ID {row.get('id')} (Status: {row.get('status')})\")\n",
        "    return is_valid\n",
        "\n",
        "def enrich_tax(row: Dict) -> Dict:\n",
        "    ''' enrich data with tax\n",
        "        INPUT: dictionary\n",
        "        OUTPUT: dictionary (enriched)\n",
        "    '''\n",
        "    new_row = row.copy()\n",
        "    price = new_row.get(\"price\", 0)\n",
        "    new_row[\"tax\"] = round(price * 0.2, 2)\n",
        "    new_row[\"total\"] = price + new_row[\"tax\"]\n",
        "    return new_row\n",
        "\n",
        "# ==========================================\n",
        "# 3. LOAD / AGGREGATE\n",
        "# ==========================================\n",
        "def save_to_json(data: List[Dict], filename: str):\n",
        "    \"\"\"Simulates loading to a specific storage bucket.\n",
        "       INPUT: A dataset and a filename\n",
        "       OUTPUT: None (as this is a simulation)\n",
        "    \"\"\"\n",
        "    logger.info(f\"Writing {len(data)} records to storage: {filename}\")\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "def calculate_revenue_reducer(acc: float, row: Dict) -> float:\n",
        "    ''' total revenue aggregator (reduce)\n",
        "        INPUT: accumulator and dictionary\n",
        "        OUTPUT: float (total)\n",
        "    '''\n",
        "    return acc + row.get(\"total\", 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we switch roles. We are the Platform. We will run three separate programs (Cells), simulating distinct microservices that react to files appearing.\n",
        "\n",
        "### Microservice A: The Ingestion Service\n",
        "A scheduled (timed - e.g. cron) job runs this service. The service fetches data and drops it into the \"Landing Zone.\"\n",
        "\n",
        "Note, we import all of the functions we created and stored in `utilities.py`."
      ],
      "metadata": {
        "id": "qMFEnIW4r_kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utilities as utils\n",
        "import os\n",
        "\n",
        "# Create directory structure to simulate S3 buckets\n",
        "os.makedirs(\"data/landing_zone\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "print(\"SERVICE STARTED: Ingestion\") # would be sent to logging but just illustratively\n",
        "\n",
        "# 1. Run Extraction\n",
        "raw_data = utils.extract_data_from_source()\n",
        "\n",
        "# 2. Emit Event (Save File)\n",
        "# In a real system, saving this file would trigger an AWS S3 Event Notification\n",
        "output_path = \"data/landing_zone/batch_001.json\"\n",
        "utils.save_to_json(raw_data, output_path)\n",
        "\n",
        "print(f\"EVENT EMITTED: New file created at {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MOv515Rr_4b",
        "outputId": "cf69d244-dbbc-4477-ce00-43cf5c142c12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-10 22:35:56,368 - ETL_Service - INFO - Connecting to external source...\n",
            "INFO:ETL_Service:Connecting to external source...\n",
            "2026-02-10 22:35:56,371 - ETL_Service - INFO - Extraction successful. Received 5 records.\n",
            "INFO:ETL_Service:Extraction successful. Received 5 records.\n",
            "2026-02-10 22:35:56,373 - ETL_Service - INFO - Writing 5 records to storage: data/landing_zone/batch_001.json\n",
            "INFO:ETL_Service:Writing 5 records to storage: data/landing_zone/batch_001.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERVICE STARTED: Ingestion\n",
            "EVENT EMITTED: New file created at data/landing_zone/batch_001.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Microservice B: The Transformation Service\n",
        "Detects batch_001.json in the Landing Zone. Cleans the data and moves it to the \"Processed Zone.\""
      ],
      "metadata": {
        "id": "TT8BhPwwso5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import utilities as utils\n",
        "\n",
        "print(\"SERVICE STARTED: Transformation Worker\")\n",
        "\n",
        "input_path = \"data/landing_zone/batch_001.json\"\n",
        "output_path = \"data/processed/clean_batch_001.json\"\n",
        "\n",
        "# 1. Listen for Event (Check if file exists)\n",
        "if not os.path.exists(input_path):\n",
        "    print(\"No new events found. Sleeping...\")\n",
        "else:\n",
        "    print(f\"EVENT RECEIVED: Processing {input_path}\")\n",
        "\n",
        "    # 2. Load Data\n",
        "    with open(input_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 3. Apply Functional Pipeline (Map/Filter)\n",
        "    # Notice we chain the map/filters here using the utility functions\n",
        "    normalized = map(utils.normalize_keys, data)\n",
        "    cleaned = map(utils.clean_price, normalized)\n",
        "    valid_only = filter(utils.is_valid_transaction, cleaned)\n",
        "    enriched = map(utils.enrich_tax, valid_only)\n",
        "\n",
        "    final_dataset = list(enriched)\n",
        "\n",
        "    # 4. Emit Next Event\n",
        "    utils.save_to_json(final_dataset, output_path)\n",
        "    print(f\"EVENT EMITTED: Clean data saved to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLf_On8GspMM",
        "outputId": "c9d61b7d-3970-4cec-8407-5126b087a6da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-10 22:37:32,247 - ETL_Service - INFO - Dropping invalid row ID 102 (Status: refunded)\n",
            "INFO:ETL_Service:Dropping invalid row ID 102 (Status: refunded)\n",
            "2026-02-10 22:37:32,249 - ETL_Service - WARNING - Found NULL price for ID 105. Defaulting to 0.0\n",
            "WARNING:ETL_Service:Found NULL price for ID 105. Defaulting to 0.0\n",
            "2026-02-10 22:37:32,250 - ETL_Service - INFO - Dropping invalid row ID 105 (Status: error)\n",
            "INFO:ETL_Service:Dropping invalid row ID 105 (Status: error)\n",
            "2026-02-10 22:37:32,252 - ETL_Service - INFO - Writing 3 records to storage: data/processed/clean_batch_001.json\n",
            "INFO:ETL_Service:Writing 3 records to storage: data/processed/clean_batch_001.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERVICE STARTED: Transformation Worker\n",
            "EVENT RECEIVED: Processing data/landing_zone/batch_001.json\n",
            "EVENT EMITTED: Clean data saved to data/processed/clean_batch_001.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it found some errors and created a new version of our data (_immutability_).\n",
        "\n",
        "### Microservice C: The Analytics Service\n",
        "Detects clean_batch_001.json in the Processed Zone. Calculates revenue and updates the Dashboard.\n"
      ],
      "metadata": {
        "id": "uCYtUPditBMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import utilities as utils\n",
        "\n",
        "print(\"SERVICE STARTED: Revenue Aggregator\")\n",
        "\n",
        "input_path = \"data/processed/clean_batch_001.json\"\n",
        "\n",
        "# 1. Listen for Event\n",
        "if not os.path.exists(input_path):\n",
        "    print(\"No processed data found.\")\n",
        "else:\n",
        "    print(f\"EVENT RECEIVED: Aggregating {input_path}\")\n",
        "\n",
        "    # 2. Load Data\n",
        "    with open(input_path, 'r') as f:\n",
        "        clean_data = json.load(f)\n",
        "\n",
        "    # 3. Reduce (Aggregation)\n",
        "    total_revenue = functools.reduce(utils.calculate_revenue_reducer, clean_data, 0.0)\n",
        "\n",
        "    # 4. Final Action\n",
        "    print(\"=\"*40)\n",
        "    print(f\"FINAL REPORT: Total Revenue is ${total_revenue:,.2f}\")\n",
        "    print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCpNVj7athFt",
        "outputId": "e5a7ee37-5315-4a45-8315-dca1aec319b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERVICE STARTED: Revenue Aggregator\n",
            "EVENT RECEIVED: Aggregating data/processed/clean_batch_001.json\n",
            "========================================\n",
            "FINAL REPORT: Total Revenue is $1,818.00\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that's it! A full ETL process."
      ],
      "metadata": {
        "id": "Gg1TSbL0tq9_"
      }
    }
  ]
}